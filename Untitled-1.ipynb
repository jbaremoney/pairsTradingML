{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will go through the process of training a trading algorithm from start to finish, however, it will not have all of the code, as that would be a huge notebook, and challenging to organize and follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with an overview of pulling the data, starting with necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binance.exceptions\n",
    "from binance.client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import requests.exceptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define api keys for Binance, and connect to api. Note that you must connect to US servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APIKEY = 'rRI5...'\n",
    "SECRETKEY = '0uid...'\n",
    "\n",
    "client = Client(APIKEY, SECRETKEY, tld=\"us\")\n",
    "\n",
    "\n",
    "try:\n",
    "    accountInfo = client.get_account()\n",
    "    print(\"successfully fetched account info\")\n",
    "    print(\"Current account balance:\", accountInfo['balances'])\n",
    "except Exception as e:\n",
    "    print(\"Unable to fix because:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we define a function to pull the data and store it, catching any exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataForCoin(symbol, interval, startDate, retries=3, wait_time=5):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            # attempt to fetch the historical data\n",
    "            candles = client.get_historical_klines(symbol, interval, startDate)\n",
    "\n",
    "            # process the data if successful\n",
    "            historicalData = []\n",
    "            for candle in candles:\n",
    "                candleData = {\n",
    "                    \"timestamp\": datetime.fromtimestamp(candle[0] / 1000),\n",
    "                    \"open\": float(candle[1]),\n",
    "                    \"high\": float(candle[2]),\n",
    "                    \"low\": float(candle[3]),\n",
    "                    \"close\": float(candle[4]),\n",
    "                    \"volume\": float(candle[5])\n",
    "                }\n",
    "                historicalData.append(candleData)\n",
    "\n",
    "            # return the dataframe if the request was successful\n",
    "            return pd.DataFrame(historicalData)\n",
    "\n",
    "        except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError) as e:\n",
    "            # handle  errors\n",
    "            print(f\"Attempt {attempt + 1} failed for {symbol}: {e}\")\n",
    "            attempt += 1\n",
    "            time.sleep(wait_time)  # wait before retrying\n",
    "\n",
    "    # raise an exception if all retries failed\n",
    "    raise Exception(f\"Max retries reached for {symbol}. API call failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we loop through the currencies we will be using, and pull for each. We store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketCapCoinsFile = 'top50coinsMarketCap.txt'\n",
    "top50MCCoins = []\n",
    "with open(marketCapCoinsFile, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        top50MCCoins.append(line)\n",
    "\n",
    "\n",
    "MCdataDictionary = {}\n",
    "\n",
    "for symbol in top50MCCoins:\n",
    "    try:\n",
    "        df = getDataForCoin(symbol + \"USDT\", Client.KLINE_INTERVAL_1HOUR, \"Jan 1, 2021\")\n",
    "        df.set_index('timestamp', inplace=True)  # Set timestamp as the index\n",
    "        df.rename(columns={'close': symbol}, inplace=True)  # Rename 'close' column to symbol\n",
    "        MCdataDictionary[symbol] = df[[symbol]]  # Keep only the symbol column for merging\n",
    "        print(\"Done pulling: \", symbol)\n",
    "        time.sleep(1)\n",
    "    except binance.exceptions.BinanceAPIException:\n",
    "        print(\"invalid symbol:\" + symbol)\n",
    "\n",
    "# Preprocess each DataFrame to ensure unique and clean index\n",
    "for symbol, df in MCdataDictionary.items():\n",
    "    # Remove duplicate timestamps\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "    # Sort by timestamp for consistency\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Store back in the dictionary\n",
    "    MCdataDictionary[symbol] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run our statistical tests (correlation, cointegration) on the data we pulled. I actually had to connect to Talon to do this as the time complexity got quite big. (50C2) pairs with lots of data for each pair to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "df = pd.read_csv('top_50_crypto_data.csv') #creating df\n",
    "\n",
    "# Convert 'timestamp' to datetime and set it as the index\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# testing for correlation using Pearson\n",
    "pearsSufficientPairsList = []\n",
    "cointSufficientPairsList = []\n",
    "pairsList = []\n",
    "\n",
    "PEARSCORRTHRESHOLD = .75\n",
    "corrResults =[]\n",
    "def corrTesting():\n",
    "    for i, coin1 in enumerate(df.columns):\n",
    "        for j, coin2 in enumerate(df.columns):\n",
    "            if i<j:\n",
    "                # only want times where both coins have data\n",
    "                filtered_df = df[[coin1, coin2]].dropna()\n",
    "\n",
    "                if len(filtered_df) > 0:  # Ensure there is data to process\n",
    "                    # Pearson Correlation\n",
    "                    pearsCorrCoeff = filtered_df[coin1].corr(filtered_df[coin2], method='pearson')\n",
    "\n",
    "                    # Cointegration Test (Engle-Granger)\n",
    "                    X = sm.add_constant(filtered_df[coin2])\n",
    "                    model = sm.OLS(filtered_df[coin1], X).fit()\n",
    "                    residuals = model.resid\n",
    "\n",
    "                    # Perform ADF test on residuals\n",
    "                    adf_test = adfuller(residuals, regresults=True)\n",
    "\n",
    "                    # Store results (correlation coefficient and ADF test stats)\n",
    "                    corrResults.append((coin1, coin2, pearsCorrCoeff, adf_test[0], adf_test[1]))\n",
    "\n",
    "    return corrResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical testing was outputted to talon, and I copy pasted it into a text file. Then, we organize the pairs into 3 groups. Note that we do not want to attempt to trade pairs with a poor statistical performance, we will only attempt to trade pairs with sufficient values for correlation or cointegration or both. Here we visualize the results from talon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "totalResultsDf = pd.read_csv('cleanCorrResults.csv')\n",
    "\n",
    "def plot_corr_heatmap(data, title):\n",
    "    heatmap_data = data.pivot(index='Coin1', columns='Coin2', values='Correlation')\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pVal_heatmap(data, title):\n",
    "    p_value_matrix = data.pivot(index='Coin1', columns='Coin2', values='p-value')\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(p_value_matrix, annot=True, cmap='coolwarm', fmt=\".3f\", linewidths=0.5, cbar_kws={'label': 'p-value'})\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we group the crypto into 3 groups. Just good correlation, just good cointegration, and good both. The intuitive assumption would be that the both category will perform the best down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out the stable coins\n",
    "filteredDf = totalResultsDf.query(\"Coin1 != 'USDC' and Coin2 != 'USDC' and Coin1 != 'DAI' and Coin2 != 'DAI'\")\n",
    "\n",
    "# getting just the good correlation bad coint ... (use backticks around hyphenated key)\n",
    "justCorrDf = filteredDf.query(\"abs(Correlation) >= .75 and `p-value` >= .051\") #76 pairs\n",
    "\n",
    "#good coint bad corr\n",
    "justCointDf = filteredDf.query(\"abs(Correlation) < .75 and `p-value` <= .05\") #33 pairs\n",
    "\n",
    "# good both\n",
    "bothGoodDf = filteredDf.query(\"abs(Correlation) >=.75 and `p-value` <= .05\") #87 pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have clean, organized data, we can write a trading algorithm. Here is the logic for trading that is used in backtesting. At a high level the backtesting function applies the trading logic at every timestamp but remembers whether it's in a position currently or not. We have trading logic that uses our trained models and one that doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(dataCoin1, dataCoin2, startDate, endDate):\n",
    "    \"\"\"Filter both coin datasets by the adjusted start date and end date.\"\"\"\n",
    "    dataCoin1.index = pd.to_datetime(dataCoin1.index)\n",
    "    dataCoin2.index = pd.to_datetime(dataCoin2.index)\n",
    "\n",
    "    # find the start dates\n",
    "    start1 = dataCoin1.dropna().index.min()\n",
    "    start2 = dataCoin2.dropna().index.min()\n",
    "\n",
    "    # use the later of the two start dates\n",
    "    adjustedStartDate = max(start1, start2)\n",
    "\n",
    "    # convert startDate and endDate to Timestamps if they aren't already\n",
    "    startDate = pd.to_datetime(startDate)\n",
    "    endDate = pd.to_datetime(endDate)\n",
    "\n",
    "    # filter the data based on adjusted start date and end date\n",
    "    if adjustedStartDate > startDate:\n",
    "        dataCoin1 = dataCoin1[adjustedStartDate:endDate]\n",
    "        dataCoin2 = dataCoin2[adjustedStartDate:endDate]\n",
    "    else:\n",
    "        dataCoin1 = dataCoin1[startDate:endDate]\n",
    "        dataCoin2 = dataCoin2[startDate:endDate]\n",
    "\n",
    "    return dataCoin1, dataCoin2, adjustedStartDate\n",
    "\n",
    "\n",
    "\n",
    "def calculate_zscore(dataCoin1, dataCoin2, window):\n",
    "    \"\"\"Calculate the ratio, rolling mean, standard deviation, and z-score.\"\"\"\n",
    "    ratio = (dataCoin1 / dataCoin2).dropna()\n",
    "    ratioMean = ratio.rolling(window=window).mean()\n",
    "    ratioStd = ratio.rolling(window=window).std()\n",
    "    zScore = (ratio - ratioMean) / ratioStd\n",
    "    return ratio, zScore\n",
    "\n",
    "\n",
    "def generate_signals(zScore, entryThreshold, exitThreshold):\n",
    "    \"\"\"Generate long, short, and exit signals based on z-score thresholds.\"\"\"\n",
    "    longSignal = (zScore < -entryThreshold)\n",
    "    shortSignal = (zScore > entryThreshold)\n",
    "    exitSignal = (abs(zScore) < exitThreshold)\n",
    "\n",
    "    signals = pd.DataFrame(index=zScore.index)\n",
    "    signals['long'] = longSignal\n",
    "    signals['short'] = shortSignal\n",
    "    signals['exit'] = exitSignal\n",
    "\n",
    "    return signals\n",
    "\n",
    "\n",
    "def tradingStratWithModel(dataCoin1, dataCoin2, model, window=30, entryThreshold=2.0, exitThreshold=0.5, startDate=\"2020-01-10\"):\n",
    "    # Generate trading signals based on z-score\n",
    "    ratio = (dataCoin1 / dataCoin2).dropna()\n",
    "    ratioMean = ratio.rolling(window=window).mean()\n",
    "    ratioStd = ratio.rolling(window=window).std()\n",
    "    zScore = (ratio - ratioMean) / ratioStd\n",
    "\n",
    "    longSignal = (zScore < -entryThreshold)\n",
    "    shortSignal = (zScore > entryThreshold)\n",
    "    exitSignal = (abs(zScore) < exitThreshold)\n",
    "\n",
    "    # Create a DataFrame to store signals\n",
    "    signals = pd.DataFrame(index=ratio.index)\n",
    "    signals['long'] = longSignal\n",
    "    signals['short'] = shortSignal\n",
    "    signals['exit'] = exitSignal\n",
    "\n",
    "    # Generate features for each potential trade\n",
    "    features_list = []\n",
    "    for i in range(len(signals)):\n",
    "        if longSignal.iloc[i] or shortSignal.iloc[i]:\n",
    "            feature = {\n",
    "                'entry_zscore': zScore.iloc[i],\n",
    "                'time_in_position': 0,  # Placeholder, time in position will be tracked during backtesting\n",
    "                'rolling_mean': ratioMean.iloc[i],\n",
    "                'rolling_std': ratioStd.iloc[i]\n",
    "            }\n",
    "            features_list.append(feature)\n",
    "\n",
    "    features_df = pd.DataFrame(features_list)\n",
    "\n",
    "    # Use the model to predict profitable trades\n",
    "    if not features_df.empty:\n",
    "        predictions = model.predict(features_df)\n",
    "        signals['predicted_profitable'] = predictions\n",
    "    else:\n",
    "        signals['predicted_profitable'] = False\n",
    "\n",
    "    # Filter signals based on model predictions\n",
    "    signals['long'] = signals['long'] & (signals['predicted_profitable'] == 1)\n",
    "    signals['short'] = signals['short'] & (signals['predicted_profitable'] == 1)\n",
    "\n",
    "    return signals, zScore, ratio\n",
    "\n",
    "\n",
    "def tradingStrat(dataCoin1, dataCoin2, window=30, entryThreshold=2.0, exitThreshold=0.5, startDate=\"2020-01-01\",\n",
    "                 endDate=\"2024-11-01\"):\n",
    "    \"\"\"Generate trading signals based on the z-score of the price ratio.\"\"\"\n",
    "    # filter the data by date range\n",
    "    dataCoin1, dataCoin2, adjustedStartDate = filter_data(dataCoin1, dataCoin2, startDate, endDate)\n",
    "\n",
    "    #Calculate z-score\n",
    "    ratio, zScore = calculate_zscore(dataCoin1, dataCoin2, window)\n",
    "\n",
    "    #generate signals\n",
    "    signals = generate_signals(zScore, entryThreshold, exitThreshold)\n",
    "\n",
    "    return signals, zScore, ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the backtesting function, not the cleanest code ever written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(pair, dataCoin1, dataCoin2, signals, initialCapital=10000, timeLimit=1000, startDate=\"2020-01-01\",\n",
    "             endDate=\"2024-01-01\"):\n",
    "    \"\"\"Backtest the trading strategy and return the results and final capital.\"\"\"\n",
    "    # filter the data by date range\n",
    "    dataCoin1, dataCoin2, adjustedStartDate = filter_data(dataCoin1, dataCoin2, startDate, endDate)\n",
    "    signals = signals[adjustedStartDate:endDate]\n",
    "\n",
    "    # combine and drop NaN values\n",
    "    combined_df = pd.concat([dataCoin1, dataCoin2, signals], axis=1).dropna()\n",
    "    if combined_df.empty:\n",
    "        print(\"No data available after filtering.\")\n",
    "        return pd.DataFrame(), initialCapital, []\n",
    "\n",
    "    # separate data again\n",
    "    dataCoin1 = combined_df.iloc[:, 0]\n",
    "    dataCoin2 = combined_df.iloc[:, 1]\n",
    "    signals = combined_df.iloc[:, 2:]\n",
    "\n",
    "    # initialize variables\n",
    "    capital = initialCapital\n",
    "    position = 0\n",
    "    capitalHistory = []\n",
    "    tradeReturns = []\n",
    "\n",
    "    for i in range(1, len(signals)):\n",
    "        priceCoin1 = dataCoin1.iloc[i]\n",
    "        priceCoin2 = dataCoin2.iloc[i]\n",
    "        currentTimeIndex = i\n",
    "\n",
    "        # enter long position\n",
    "        if position == 0 and signals['long'].iloc[i]:\n",
    "            position = 1\n",
    "            entryPriceCoin1 = priceCoin1\n",
    "            entryPriceCoin2 = priceCoin2\n",
    "            investment = 0.25 * capital\n",
    "            entryTimeIndex = i\n",
    "\n",
    "        # enter short position\n",
    "        elif position == 0 and signals['short'].iloc[i]:\n",
    "            position = -1\n",
    "            entryPriceCoin1 = priceCoin1\n",
    "            entryPriceCoin2 = priceCoin2\n",
    "            investment = 0.25 * capital\n",
    "            entryTimeIndex = i\n",
    "\n",
    "        # exit long position\n",
    "        elif position == 1 and (signals['exit'].iloc[i] or (currentTimeIndex - entryTimeIndex > timeLimit)):\n",
    "            exitPriceCoin1 = priceCoin1\n",
    "            exitPriceCoin2 = priceCoin2\n",
    "            tradeReturn = (exitPriceCoin1 - entryPriceCoin1) / entryPriceCoin1 * investment - \\\n",
    "                          (exitPriceCoin2 - entryPriceCoin2) / entryPriceCoin2 * investment\n",
    "            capital += tradeReturn\n",
    "            tradeReturns.append(tradeReturn)\n",
    "            position = 0\n",
    "\n",
    "        # exit short position\n",
    "        elif position == -1 and (signals['exit'].iloc[i] or (currentTimeIndex - entryTimeIndex > timeLimit)):\n",
    "            exitPriceCoin1 = priceCoin1\n",
    "            exitPriceCoin2 = priceCoin2\n",
    "            tradeReturn = (entryPriceCoin2 - exitPriceCoin2) / entryPriceCoin2 * investment - \\\n",
    "                          (entryPriceCoin1 - exitPriceCoin1) / entryPriceCoin1 * investment\n",
    "            capital += tradeReturn\n",
    "            tradeReturns.append(tradeReturn)\n",
    "            position = 0\n",
    "\n",
    "        capitalHistory.append(capital)\n",
    "\n",
    "    # create results DataFrame\n",
    "    results = pd.DataFrame(index=signals.index[1:])\n",
    "    results['Capital'] = capitalHistory\n",
    "\n",
    "    return results, capital, tradeReturns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining all these functions, we use the default logic's performance to generate training data for the ML algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_training_data(dataCoin1, dataCoin2, signals, window=30, maxHoldTime=50):\n",
    "    features = []\n",
    "    labels = []\n",
    "    tradeReturns = []\n",
    "\n",
    "    # Calculate rolling statistics and z-score\n",
    "    ratio = (dataCoin1 / dataCoin2).dropna()\n",
    "    ratioMean = ratio.rolling(window=window).mean()\n",
    "    ratioStd = ratio.rolling(window=window).std()\n",
    "    zScore = (ratio - ratioMean) / ratioStd\n",
    "\n",
    "    # Iterate through signals to extract features and labels\n",
    "    position = 0\n",
    "    for i in range(1, len(signals)):\n",
    "        if position == 0:\n",
    "            if signals['long'].iloc[i]:\n",
    "                entryPrice1 = dataCoin1.iloc[i]\n",
    "                entryPrice2 = dataCoin2.iloc[i]\n",
    "                entryZScore = zScore.iloc[i]\n",
    "                position = 1\n",
    "                entryIndex = i\n",
    "\n",
    "            elif signals['short'].iloc[i]:\n",
    "                entryPrice1 = dataCoin1.iloc[i]\n",
    "                entryPrice2 = dataCoin2.iloc[i]\n",
    "                entryZScore = zScore.iloc[i]\n",
    "                position = -1\n",
    "                entryIndex = i\n",
    "\n",
    "        elif position != 0:\n",
    "            # Track how long the position is held\n",
    "            timeInPosition = i - entryIndex\n",
    "\n",
    "            # Close position based on maxHoldTime or exit signal\n",
    "            if signals['exit'].iloc[i] or timeInPosition >= maxHoldTime:\n",
    "                exitPrice1 = dataCoin1.iloc[i]\n",
    "                exitPrice2 = dataCoin2.iloc[i]\n",
    "\n",
    "                # Calculate trade return\n",
    "                if position == 1:\n",
    "                    tradeReturn = (exitPrice1 - entryPrice1) / entryPrice1 - (exitPrice2 - entryPrice2) / entryPrice2\n",
    "                else:\n",
    "                    tradeReturn = (entryPrice2 - exitPrice2) / entryPrice2 - (entryPrice1 - exitPrice1) / entryPrice1\n",
    "\n",
    "                # Create features for this trade\n",
    "                feature = {\n",
    "                    'entry_zscore': entryZScore,\n",
    "                    'time_in_position': timeInPosition,\n",
    "                    'price_ratio': entryPrice1 / entryPrice2,\n",
    "                    'rolling_mean': ratioMean.iloc[entryIndex],\n",
    "                    'rolling_std': ratioStd.iloc[entryIndex],\n",
    "                }\n",
    "                features.append(feature)\n",
    "                labels.append(1 if tradeReturn > 0 else 0)  # 1 for profit, 0 for loss\n",
    "                tradeReturns.append(tradeReturn)\n",
    "\n",
    "                # Reset position\n",
    "                position = 0\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    features_df = pd.DataFrame(features)\n",
    "    labels_df = pd.Series(labels, name='label')\n",
    "\n",
    "    return features_df, labels_df, tradeReturns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a few classification algorithms to try to help identify when trades will be successful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "# Load the training data from CSV\n",
    "df = pd.read_csv(\"training_data.csv\")\n",
    "\n",
    "# Drop 'price_ratio' and 'pair' columns\n",
    "df.drop(columns=['price_ratio', 'pair'], inplace=True)\n",
    "\n",
    "# Split features and labels\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "# Standardize the features (important for models like Logistic Regression, SVM, and KNN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dictionary of models to try\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "\n",
    "\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "    model_filename = f'{model_name}_model.pkl'\n",
    "    joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluating Logistic Regression...\n",
    "Accuracy: 0.6603\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.66      1.00      0.80     14617\n",
    "           1       0.42      0.00      0.01      7507\n",
    "\n",
    "    accuracy                           0.66     22124\n",
    "   macro avg       0.54      0.50      0.40     22124\n",
    "weighted avg       0.58      0.66      0.53     22124\n",
    "\n",
    "--------------------------------------------------\n",
    "Training and evaluating Random Forest...\n",
    "Accuracy: 0.6515\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.70      0.82      0.76     14617\n",
    "           1       0.48      0.32      0.39      7507\n",
    "\n",
    "    accuracy                           0.65     22124\n",
    "   macro avg       0.59      0.57      0.57     22124\n",
    "weighted avg       0.63      0.65      0.63     22124\n",
    "\n",
    "--------------------------------------------------\n",
    "Training and evaluating K-Nearest Neighbors...\n",
    "Accuracy: 0.6166\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.68      0.78      0.73     14617\n",
    "           1       0.41      0.29      0.34      7507\n",
    "\n",
    "    accuracy                           0.62     22124\n",
    "   macro avg       0.55      0.54      0.53     22124\n",
    "weighted avg       0.59      0.62      0.60     22124\n",
    "\n",
    "--------------------------------------------------\n",
    "Training and evaluating Gradient Boosting...\n",
    "Accuracy: 0.6767\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.69      0.94      0.79     14617\n",
    "           1       0.59      0.16      0.25      7507\n",
    "\n",
    "    accuracy                           0.68     22124\n",
    "   macro avg       0.64      0.55      0.52     22124\n",
    "weighted avg       0.65      0.68      0.61     22124\n",
    "\n",
    "--------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is much harder for these models to learn when a trade will be profitable as opposed to non profitable. This may be due to imbalance, there are more non-profitable trades. This is not quite as easy to solve in our context, as we do not want to predict false profitable trades because I will lose money.\n",
    "\n",
    "Notice the models are saved now so we can compare the performance of the algorithm before and after using the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any classification algorithms and just messing with the thresholds and stop loss based on intuition, these were some of the results.\n",
    "\n",
    "Results from initial backtesting:\n",
    "bothGoodDf: Mostly bad results, besides Some eth pairs, eth/sol 45% over 2023-2024, 22% over the 4 years only significantly positive pair. Z score thresholds all kept it relatively positive. Average return around 6% when changing z scores within reasonable distances (0.5 exit, 2.0 enter ) being standard.\n",
    "\n",
    "justCoinDf: Also mostly bad results, but BTC and LINK 380% increase over 2023-2024, Hard to say whether these are due to underlying cointegration or just chance. Average return around 3%\n",
    "\n",
    "justCorrDf: Not many positive pairs. Average return -2%\n",
    "\n",
    "\n",
    "Now that the models have been trained, I'll test the strategy with the random forest model, because that performed the best of the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputBacktestResults(pairSetDf, string):\n",
    "    # Define start and end dates\n",
    "    startDate = '2021-12-31'\n",
    "    endDate = '2024-11-01'\n",
    "\n",
    "    # Lists to store results\n",
    "    results_no_model = []\n",
    "    results_with_model = []\n",
    "\n",
    "    # Iterate through the pairs in bothGoodDf\n",
    "    for i in range(len(pairSetDf)):\n",
    "        pair = (pairSetDf['Coin1'].iloc[i], pairSetDf['Coin2'].iloc[i])\n",
    "        print(\"Processing Pair:\", pair)\n",
    "\n",
    "        # Get price data for the pair\n",
    "        dataCoin1 = df[pair[0]]\n",
    "        dataCoin2 = df[pair[1]]\n",
    "\n",
    "        ### Backtest Without Model\n",
    "        signals_no_model, _, _ = tradingStrat(dataCoin1, dataCoin2, startDate=startDate, endDate=endDate)\n",
    "        _, finalCapital_no_model, tradeReturns_no_model = backtest(pair, dataCoin1, dataCoin2, signals_no_model,\n",
    "                                                                   startDate=startDate, endDate=endDate)\n",
    "        avg_return_no_model = sum(tradeReturns_no_model) / len(tradeReturns_no_model) if tradeReturns_no_model else 0\n",
    "        results_no_model.append(\n",
    "            {'pair': f\"{pair[0]}-{pair[1]}\", 'final_capital': finalCapital_no_model, 'avg_return': avg_return_no_model})\n",
    "\n",
    "        ### Backtest With Model\n",
    "        signals_with_model, _, _ = tradingStratWithModel(dataCoin1, dataCoin2, model, startDate=startDate)\n",
    "        _, finalCapital_with_model, tradeReturns_with_model = backtest(pair, dataCoin1, dataCoin2, signals_with_model,\n",
    "                                                                       startDate=startDate, endDate=endDate)\n",
    "        avg_return_with_model = sum(tradeReturns_with_model) / len(\n",
    "            tradeReturns_with_model) if tradeReturns_with_model else 0\n",
    "        results_with_model.append({'pair': f\"{pair[0]}-{pair[1]}\", 'final_capital': finalCapital_with_model,\n",
    "                                   'avg_return': avg_return_with_model})\n",
    "\n",
    "    # Convert results to DataFrames\n",
    "    results_no_model_df = pd.DataFrame(results_no_model)\n",
    "    results_with_model_df = pd.DataFrame(results_with_model)\n",
    "\n",
    "    # Merge the results for comparison\n",
    "    comparison_df = pd.merge(results_no_model_df, results_with_model_df, on='pair',\n",
    "                                      suffixes=('_no_model', '_with_model'))\n",
    "\n",
    "    # Display the comparison\n",
    "\n",
    "\n",
    "    # Save to CSV for further analysis\n",
    "    comparison_df.to_csv(f'{string}trading_strategy_comparison.csv', index=False)\n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "bothGoodResults = outputBacktestResults(bothGoodDf, \"bothGood\")\n",
    "justCorrResults = outputBacktestResults(justCorrDf, \"justCorr\")\n",
    "justCointResults = outputBacktestResults(justCointDf, \"justCoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we'll have data showing how each group performed without using a model and with using a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the CSV files for the two strategies\n",
    "both_good_df = pd.read_csv(\"bothGoodtrading_strategy_comparison.csv\")\n",
    "just_coint_df = pd.read_csv(\"justCointtrading_strategy_comparison.csv\")\n",
    "just_corr_df = pd.read_csv(\"justCorrtrading_strategy_comparison.csv\")\n",
    "\n",
    "# Add a 'group' column to identify each dataset\n",
    "both_good_df['group'] = 'Both Good'\n",
    "just_coint_df['group'] = 'Just Coint'\n",
    "just_corr_df['group'] = 'Just Corr'\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "print(\"Both Good Data:\")\n",
    "print(both_good_df.head())\n",
    "\n",
    "print(\"\\nJust Coint Data:\")\n",
    "print(just_coint_df.head())\n",
    "\n",
    "print(\"\\nJust Corr Data:\")\n",
    "print(just_corr_df.head())\n",
    "\n",
    "# Combine all three DataFrames into a single DataFrame for analysis\n",
    "combined_df = pd.concat([both_good_df, just_coint_df, just_corr_df], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"\\nCombined Data:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "# Summary Statistics Function\n",
    "def display_summary_stats(df):\n",
    "    summary = df.groupby('group').agg({\n",
    "        'final_capital_no_model': ['mean', 'std'],\n",
    "        'avg_return_no_model': ['mean', 'std'],\n",
    "        'final_capital_with_model': ['mean', 'std'],\n",
    "        'avg_return_with_model': ['mean', 'std']\n",
    "    })\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(summary)\n",
    "\n",
    "# Display the summary statistics\n",
    "display_summary_stats(combined_df)\n",
    "\n",
    "# Visualization Functions\n",
    "# Final Capital Comparison Plot\n",
    "def plot_final_capital(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df, x='group', y='final_capital_no_model', errorbar=None, label='No Model')\n",
    "    sns.barplot(data=df, x='group', y='final_capital_with_model', errorbar=None, label='With Model', alpha=0.7)\n",
    "    plt.title('Final Capital Comparison by Group')\n",
    "    plt.ylabel('Final Capital ($)')\n",
    "    plt.xlabel('Group')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Average Return Comparison Plot\n",
    "def plot_average_return(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df, x='group', y='avg_return_no_model', errorbar=None, label='No Model')\n",
    "    sns.barplot(data=df, x='group', y='avg_return_with_model', errorbar=None, label='With Model', alpha=0.7)\n",
    "    plt.title('Average Return Comparison by Group')\n",
    "    plt.ylabel('Average Return')\n",
    "    plt.xlabel('Group')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Boxplots for Final Capital and Average Return\n",
    "def plot_boxplots(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df, x='group', y='final_capital_no_model')\n",
    "    sns.boxplot(data=df, x='group', y='final_capital_with_model')\n",
    "    plt.title('Distribution of Final Capital by Group')\n",
    "    plt.ylabel('Final Capital ($)')\n",
    "    plt.xlabel('Group')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df, x='group', y='avg_return_no_model')\n",
    "    sns.boxplot(data=df, x='group', y='avg_return_with_model')\n",
    "    plt.title('Distribution of Average Return by Group')\n",
    "    plt.ylabel('Average Return')\n",
    "    plt.xlabel('Group')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting functions\n",
    "plot_final_capital(combined_df)\n",
    "plot_average_return(combined_df)\n",
    "plot_boxplots(combined_df)\n",
    "\n",
    "\n",
    "# Save the summary statistics to a new CSV file\n",
    "def save_summary_stats(df, filename=\"combined_strategy_summary.csv\"):\n",
    "    summary = df.groupby('group').agg({\n",
    "        'final_capital_no_model': ['mean', 'std'],\n",
    "        'avg_return_no_model': ['mean', 'std'],\n",
    "        'final_capital_with_model': ['mean', 'std'],\n",
    "        'avg_return_with_model': ['mean', 'std']\n",
    "    })\n",
    "    summary.to_csv(filename)\n",
    "    print(f\"\\nSummary statistics saved to {filename}\")\n",
    "\n",
    "# Save the summary statistics\n",
    "save_summary_stats(combined_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
